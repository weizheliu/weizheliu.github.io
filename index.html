<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-KYQ6ZE9BDD"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-KYQ6ZE9BDD');
</script>

  <title>Weizhe Liu</title>
  
  <meta name="author" content="Weizhe Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/tencent.jpeg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Weizhe Liu</name>
              </p>
              <p>I am a researcher at Bytedance, working on GenAI.
              </p>
              <p>
                 Prior to that, I was a senior research scientist and tech lead at Tencent. I obtained my Ph.D. from <a href="https://www.epfl.ch/en/">École Polytechnique Fédérale de Lausanne</a> (EPFL) under the supervision of <a href="https://people.epfl.ch/pascal.fua/bio?lang=en">Prof. Pascal Fua</a> in 2021. I received the Master of Science degree from EPFL in 2017 and the Bachelor of Engineering degree from <a href="https://en.uestc.edu.cn/">University of Electronic Science and Technology of China</a> (UESTC) in 2014. 
              </p>
              <p style="text-align:center">
                <a href="mailto:weizheliu1991@163.com">Email</a> &nbsp/&nbsp
                <a href="data/cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=p351VxAAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/weizhe-liu-68873589/"> LinkedIn  &nbsp/&nbsp
                <a href="https://twitter.com/weizheliu1991/"> Twitter </a>
              </p>
            </td>
            <td style="padding:5.0%;width:70%;max-width:70%">
              <a href="images/weizhe_liu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/weizhe_liu.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Open Positions</heading>
            <p>
              We have a limited number of positions for <font color="red">research interns</font> in Beijing/Shanghai/Hangzhou, the research topics are 3D Vision, AIGC(image/video/3D) and other related ones. This position is target for publications in top-tier CV/CG conferences and only for Ph.D. students who are still enrolled in an university. If you are interested in this position, please send your resume to my <a href = "mailto: weizheliu1991@163.com">email</a>.
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Interests</heading>
              <p>
              My research interests lie in the field of Computer Vision, Machine Learning and Robotics. My current focus is GenAI and 3D vision.  Before that, I worked on developing algorithms to crowd analysis problem, including counting, localization and motion estimation. I also worked on video understanding, action recognition, semantic segmentation, domain adaptation and learning with less supervision.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Publications</heading>
            </td>
          </tr>
          <tr>
            <td>
              * indicates equal contribution, † represents project lead and & denotes corresponding author.
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="Pandora3D_stop()" onmouseover="Pandora3D_start()"></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class='hidden' id='Pandora3D'><img src='images/pandora.png' width="180" height="160"></div>
          </td>
               <td style="padding:20px;width:75%;vertical-align:middle">
                 <a href="https://arxiv.org/pdf/2403.16210">
                   <papertitle>Pandora3D: A Comprehensive Framework for High-Quality 3D Shape and Texture Generation</papertitle>
                 </a>
                 <br>
                 Jiayu Yang<sup>*</sup>, Taizhang Shang<sup>*</sup>, Weixuan Sun<sup>*</sup>, Xibin Song<sup>*</sup>, Ziang Chen<sup>*</sup>, Senbo Wang<sup>*</sup>, Shenzhou Chen<sup>*</sup>,<strong> Weizhe Liu<sup>*</sup></strong>, Hongdong Li, Pan Ji
                 <br>
                 <strong><em>Tech Report </em>, 2025</strong>
                 <br>
                 <a href="https://arxiv.org/pdf/2502.14247">pdf</a>/
                 <a href="https://github.com/Tencent/Tencent-XR-3DGen">code</a> 
                 <p></p>
                 <p> This report presents a comprehensive framework for generating high-quality 3D shapes and textures from diverse input prompts, including single images, multi-view images, and text descriptions. This report details the system architecture, experimental results, and potential future directions to improve and expand the framework.
                  </p>
               </td>
             </tr>

          <tr onmouseout="mars_stop()" onmouseover="mars_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class='hidden' id='mars'><img src='images/mars.png' width="180" height="160"></div>
            </td>
                 <td style="padding:20px;width:75%;vertical-align:middle">
                   <a href="https://arxiv.org/pdf/2403.16210">
                     <papertitle>MARS: Mesh AutoRegressive Model for 3D Shape Detailization</papertitle>
                   </a>
                   <br>
                   Jingnan Gao, <strong>Weizhe Liu <sup>†</sup></strong>, Weixuan Sun, Senbo Wang, Xibin Song, Taizhang Shang, Shenzhou Chen, Hongdong Li, Xiaokang Yang, Yichao Yan, Pan Ji
                   <br>
                   <strong><em>arXiv preprint </em>, 2025</strong>
                   <br>
                   <a href="https://arxiv.org/pdf/2502.11390">pdf</a>
                   <p></p>
                   <p> In this paper, we introduce MARS, a novel approach for 3D shape detailization. Our method capitalizes on a novel multi-LOD, multi-category mesh representation to learn shape-consistent mesh representations in latent space across different LODs.
                    </p>
                 </td>
               </tr>

          <tr onmouseout="neurips_2024_stop()" onmouseover="neurips_2024_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class='hidden' id='neurips_2024'><img src='images/neurips_2024.jpg' width="180" height="160"></div>
            </td>
                 <td style="padding:20px;width:75%;vertical-align:middle">
                   <a href="https://arxiv.org/pdf/2403.16210">
                     <papertitle>LAM3D: Large Image-Point-Cloud Alignment Model for 3D Reconstruction from Single Image</papertitle>
                   </a>
                   <br>
                   Ruikai Cui, Xibin Song, Weixuan Sun, Senbo Wang, <strong>Weizhe Liu</strong>, Shenzhou Chen, Taizhang Shang, Yang Li, Nick Barnes, Hongdong Li, Pan Ji
                   <br>
                   <strong><em>Neural Information Processing Systems (NeurIPS) </em>, 2024</strong>
                   <br>
                   <a href="https://arxiv.org/pdf/2405.15622">pdf</a>
                   <p></p>
                   <p> Large Reconstruction Models have made significant strides in the realm of automated 3D content generation from single or multiple input images. Despite their success, these models often produce 3D meshes with geometric inaccuracies, stemming from the inherent challenges of deducing 3D shapes solely from image data. In this work, we introduce a novel framework, the Large Image and Point Cloud Alignment Model (LAM3D), which utilizes 3D point cloud data to enhance the fidelity of generated 3D meshes.
                    </p>
                 </td>
               </tr>
          
          
          <tr onmouseout="siggraph_asia_2024_stop()" onmouseover="siggraph_asia_2024_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class='hidden' id='siggraph_asia_2024'><img src='images/siggraph_asia_2024.jpg' width="180" height="160"></div>
            </td>
                 <td style="padding:20px;width:75%;vertical-align:middle">
                   <a href="https://arxiv.org/pdf/2403.16210">
                     <papertitle>Frankenstein: Generating Semantic-Compositional 3D Scenes in One Tri-Plane</papertitle>
                   </a>
                   <br>
                   Han Yan, Yang Li, Zhennan Wu, Shenzhou Chen, Weixuan Sun, Taizhang Shang, <strong>Weizhe Liu</strong>, Tian Chen, Xiaqiang Dai, Chao Ma, Hongdong Li, Pan Ji
                   <br>
                   <strong><em>SIGGRAPH Asia</em>, 2024</strong>
                   <br>
                   <a href="https://www.youtube.com/watch?v=lRn-HqyCrLI">video</a>/
                   <a href="https://arxiv.org/pdf/2403.16210">pdf</a>
                   <p></p>
                   <p> We present Frankenstein, a diffusion-based framework that can generate semantic-compositional 3D scenes in a single pass. Unlike existing methods that output a single, unified 3D shape, Frankenstein simultaneously generates multiple separated shapes, each corresponding to a semantically meaningful part. The 3D scene information is encoded in one single tri-plane tensor, from which multiple Singed Distance Function (SDF) fields can be decoded to represent the compositional shapes.
                    </p>
                 </td>
               </tr>


          <tr onmouseout="eccv2024_stop()" onmouseover="eccv2024_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class='hidden' id='eccv2024'><img src='images/eccv2024.jpg' width="180" height="160"></div>
            </td>
                 <td style="padding:20px;width:75%;vertical-align:middle">
                   <a href="https://arxiv.org/pdf/2403.18241">
                     <papertitle>NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation</papertitle>
                   </a>
                   <br>
                   Ruikai Cui, <strong>Weizhe Liu <sup>&</sup></strong>, Weixuan Sun, Senbo Wang, Taizhang Shang, Yang Li, Xibin Song, Han Yan, Zhennan Wu, Shenzhou Chen, Hongdong Li, Pan Ji
                   <br>
                   <strong><em>The European Conference on Computer Vision (ECCV)</em>, 2024</strong>
                   <br>
                   <a href="https://weizheliu.github.io/NeuSDFusion/">project page</a>/
                   <a href="https://weizheliu.github.io/NeuSDFusion/static/pdfs/neusdfusion.pdf">pdf</a>/
                   <a href="https://github.com/Tencent/Tencent-XR-3DGen/tree/main/geometry/neusdfusion">code</a> 
                   <p></p>
                   <p> 3D shape generation aims to produce innovative 3D content adhering to specific conditions and constraints. Existing methods often decompose 3D shapes into a sequence of localized components, treating each element in isolation without considering spatial consistency. As a result, these approaches exhibit limited versatility in 3D data representation and shape generation, hindering their ability to generate highly diverse 3D shapes that comply with the specified constraints. In this paper, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling.
                    </p>
                 </td>
               </tr>

          <tr onmouseout="siggraph2024_stop()" onmouseover="siggraph2024_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class='hidden' id='siggraph2024'><img src='images/siggraph2024.jpg' width="180" height="160"></div>
                <!-- <div id='wacv23_still'>
                    <a href='images/wacv23_still.png' width="180" height="160" ><img src='images/wacv23.png' width="180" height="160"></a>
              </div> -->
                <!-- <script type="text/javascript">
                function wacv23_start() {
                  document.getElementById('wacv23').style.display = 'inline';
                  document.getElementById('wacv23_still').style.display = 'none';
                }

                function wacv23_stop() {
                  document.getElementById('wacv23').style.display = 'none';
                  document.getElementById('wacv_still').style.display = 'inline';
                }
                wacv23_stop()
              </script> -->
            </td>
                 <td style="padding:20px;width:75%;vertical-align:middle">
                   <a href="https://arxiv.org/pdf/2401.17053">
                     <papertitle>BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation</papertitle>
                   </a>
                   <br>
                   Zhennan Wu, Yang Li, Han Yan, Taizhang Shang, Weixuan Sun, Senbo Wang, Ruikai Cui, <strong>Weizhe Liu</strong>, Hiroyuki Sato, Hongdong Li, Pan Ji
                   <br>
                   <strong><em>ACM Transactions on Graphics (SIGGRAPH)</em>, 2024</strong>
                   <br>
                   <a href="https://yang-l1.github.io/blockfusion/">project page</a>/
                   <a href="https://arxiv.org/pdf/2401.17053">pdf</a>/
                   <a href="https://github.com/Tencent/BlockFusion/">code</a> 
                   <p></p>
                   <p> We present BlockFusion, a diffusion-based model that generates 3D scenes as unit blocks and seamlessly incorporates new blocks to extend the scene. BlockFusion is trained using datasets of 3D blocks that are randomly cropped from complete 3D scene meshes. Through per-block fitting, all training blocks are converted into the hybrid neural fields: with a tri-plane containing the geometry features, followed by a Multi-layer Perceptron (MLP) for decoding the signed distance values.
                    </p>
                 </td>
               </tr>


               <tr onmouseout="icra23_stop()" onmouseover="icra23_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class='hidden' id='icra23'><img src='images/icra23.jpg' width="180" height="160"></div>
                    <!-- <div id='wacv23_still'>
                        <a href='images/wacv23_still.png' width="180" height="160" ><img src='images/wacv23.png' width="180" height="160"></a>
                  </div> -->
                    <!-- <script type="text/javascript">
                    function wacv23_start() {
                      document.getElementById('wacv23').style.display = 'inline';
                      document.getElementById('wacv23_still').style.display = 'none';
                    }
    
                    function wacv23_stop() {
                      document.getElementById('wacv23').style.display = 'none';
                      document.getElementById('wacv_still').style.display = 'inline';
                    }
                    wacv23_stop()
                  </script> -->
                </td>
                     <td style="padding:20px;width:75%;vertical-align:middle">
                       <a href="https://ieeexplore.ieee.org/abstract/document/10611723/">
                         <papertitle>RGB-based Category-level Object Pose Estimation via Decoupled Metric Scale Recovery</papertitle>
                       </a>
                       <br>
                       Jiaxin Wei, Xibin Song, <strong>Weizhe Liu</strong>, Laurent Kneip, Hongdong Li, Pan Ji
                       <br>
                       <strong><em>IEEE International Conference on Robotics and Automation(ICRA)</em>, 2023</strong>
                       <br>
                       <a href="https://arxiv.org/pdf/2309.10255">pdf</a>/
                       <a href="https://github.com/goldoak/DMSR">code</a> 
                       <p></p>
                       <p> While showing promising results, recent RGB-D camera-based category-level object pose estimation methods have restricted applications due to the heavy reliance on depth sensors. RGB-only methods provide an alternative to this problem yet suffer from inherent scale ambiguity stemming from monocular observations. In this paper, we propose a novel pipeline that decouples the 6D pose and size estimation to mitigate the influence of imperfect scales on rigid transformations.
                        </p>
                     </td>
                   </tr>

          <tr onmouseout="wacv23_stop()" onmouseover="wacv23_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class='hidden' id='wacv23'><img src='images/wacv23.png' width="180" height="160"></div>
                <!-- <div id='wacv23_still'>
                    <a href='images/wacv23_still.png' width="180" height="160" ><img src='images/wacv23.png' width="180" height="160"></a>
              </div> -->
                <!-- <script type="text/javascript">
                function wacv23_start() {
                  document.getElementById('wacv23').style.display = 'inline';
                  document.getElementById('wacv23_still').style.display = 'none';
                }

                function wacv23_stop() {
                  document.getElementById('wacv23').style.display = 'none';
                  document.getElementById('wacv_still').style.display = 'inline';
                }
                wacv23_stop()
              </script> -->
            </td>
                 <td style="padding:20px;width:75%;vertical-align:middle">
                   <a href="https://arxiv.org/pdf/2111.09301.pdf">
                     <papertitle>Multi-view Tracking Using Weakly Supervised Human Motion Prediction</papertitle>
                   </a>
                   <br>
                   Martin Engilberge, <strong>Weizhe Liu</strong>, Pascal Fua
                   <br>
                   <strong><em>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, 2023</strong>
                   <br>
                   <a href="https://arxiv.org/pdf/2210.10771.pdf">pdf</a>/
                   <a href="https://github.com/cvlab-epfl/MVFlow">code</a> 
                   <p></p>
                   <p> Multi-view approaches to people-tracking have the potential to better handle occlusions than single-view ones in crowded scenes. They often rely on the tracking-by-detection paradigm, which involves detecting people first and then connecting the detections. In this paper, we argue that an even more effective approach is to predict people motion over time and infer people's presence in individual frames from these. This enables to enforce consistency both over time and across views of a single temporal frame. We validate our approach on the PETS2009 and WILDTRACK datasets and demonstrate that it outperforms state-of-the-art methods.
                    </p>
                 </td>
               </tr>


          <tr onmouseout="cvpr22_2_stop()" onmouseover="cvpr22_2_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class='hidden' id='cvpr22_2'><img src='images/vava.gif' width="180" height="160"></div>
                <div id='cvpr22_2_still'>
                    <a href='images/vava.gif' width="180" height="160" ><img src='images/vava.png' width="180" height="160"></a>
              </div>
                <script type="text/javascript">
                function cvpr22_2_start() {
                  document.getElementById('cvpr22_2').style.display = 'inline';
                  document.getElementById('cvpr22_2_still').style.display = 'none';
                }

                function cvpr22_2_stop() {
                  document.getElementById('cvpr22_2').style.display = 'none';
                  document.getElementById('cvpr22_2_still').style.display = 'inline';
                }
                cvpr22_2_stop()
              </script>
            </td>
                 <td style="padding:20px;width:75%;vertical-align:middle">
                   <a href="https://arxiv.org/pdf/2111.09301.pdf">
                     <papertitle>Learning to Align Sequential Actions in the Wild</papertitle>
                   </a>
                   <br>
             <strong>Weizhe Liu</strong>, Bugra Tekin, Huseyin Coskun, Vibhav Vineet, Pascal Fua, Marc Pollefeys
                   <br>
                   <strong><em>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022</strong>
                   <br>
                   <a href="https://arxiv.org/pdf/2111.09301.pdf">pdf</a>/
                   <a href="https://github.com/weizheliu/VAVA">code</a> 
                   <p></p>
                   <p> In this paper, we propose an approach to align sequential actions in the wild that involve diverse temporal variations. To this end, we propose an approach to enforce temporal priors on the optimal transport matrix, which leverages temporal consistency, while allowing for variations in the order of actions. Our model accounts for both monotonic and non-monotonic sequences and handles background frames that should not be aligned. We demonstrate that our approach consistently outperforms the stateof-the-art in self-supervised sequential action representation learning on four different benchmark datasets.
                    </p>
                 </td>
               </tr>



          <tr onmouseout="cvpr22_1_stop()" onmouseover="cvpr22_1_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class='hidden' id='cvpr22_1'><img src='images/cvpr22_1.gif' width="180" height="160"></div>
                <div id='cvpr22_1_still'>
                    <a href='images/cvpr22_1.gif' width="180" height="160" ><img src='images/cvpr22_1_still.jpg' width="180" height="160"></a>
              </div>
                <script type="text/javascript">
                function cvpr22_1_start() {
                  document.getElementById('cvpr22_1').style.display = 'inline';
                  document.getElementById('cvpr22_1_still').style.display = 'none';
                }

                function cvpr22_1_stop() {
                  document.getElementById('cvpr22_1').style.display = 'none';
                  document.getElementById('cvpr22_1_still').style.display = 'inline';
                }
                cvpr22_1_stop()
              </script>
            </td>
                 <td style="padding:20px;width:75%;vertical-align:middle">
                   <a href="https://arxiv.org/pdf/2103.16291.pdf">
                     <papertitle>Leveraging Self-Supervision for Cross-Domain Crowd Counting</papertitle>
                   </a>
                   <br>
             <strong>Weizhe Liu</strong>, Nikita Durasov, Pascal Fua
                   <br>
                   <strong><em>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022 (<font color="red">Oral</font>)</strong>
                   <br>
                   <a href="https://arxiv.org/pdf/2103.16291.pdf">pdf</a>/
                   <a href="https://github.com/weizheliu/Cross-Domain-Crowd-Counting">code</a> 
                   <p></p>
                   <p> In this paper, we train with both synthetic images, along with their associated labels, and unlabeled real images. To this end, we force our network to learn perspective-aware features by training it to recognize upside-down real images from regular ones and incorporate into it the ability to predict its own uncertainty so that it can generate useful pseudo labels for fine-tuning purposes. This yields an algorithm that consistently outperforms state-of-the-art cross-domain crowd counting ones without any extra computation at inference time.</p>
                 </td>
               </tr>

               
          <tr onmouseout="pami21_stop()" onmouseover="pami21_start()">

            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class='hidden' id='pami21'><img src='images/pami21.gif' width="180" height="160"></div>
                <div id='pami21_still'>
                    <a href='images/pami21.gif' width="180" height="160" ><img src='images/pami21_still.png' width="180" height="160"></a>
              </div>
                <script type="text/javascript">
                function pami21_start() {
                  document.getElementById('pami21').style.display = 'inline';
                  document.getElementById('pami21_still').style.display = 'none';
                }

                function pami21_stop() {
                  document.getElementById('pami21').style.display = 'none';
                  document.getElementById('pami21_still').style.display = 'inline';
                }
                pami21_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2012.00452.pdf">
                <papertitle>Counting People by Estimating People Flows</papertitle>
              </a>
              <br>
        <strong>Weizhe Liu</strong>, Mathieu Salzmann, Pascal Fua
              <br>
              <strong><em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2021</strong>
              <br>
              <a href="https://arxiv.org/pdf/2012.00452.pdf">pdf</a> /
              <a href="https://ieeexplore.ieee.org/document/9508171/authors#authors">press</a> /
              <a href="https://www.youtube.com/watch?v=dck3HVMLtfY">video</a> /
              <a href="https://github.com/weizheliu/People-Flows">code</a> 
              <p></p>
              <p> In this paper, we advocate estimating people flows across image locations between consecutive images and inferring the people densities from these flows instead of directly regressing them. This enables us to impose much stronger constraints encoding the conservation of the number of people. As a result, it significantly boosts performance without requiring a more complex architecture. Furthermore, it allows us to exploit the correlation between people flow and optical flow to further improve the results. We also show that leveraging people conservation constraints in both a spatial and temporal manner makes it possible to train a deep crowd counting model in an active learning setting with much fewer annotations. This significantly reduces the annotation cost while still leading to similar performance to the full supervision case. </p>
            </td>
          </tr>

		
          <tr onmouseout="eccv20_stop()" onmouseover="eccv20_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class='hidden' id='eccv20'><img src='images/eccv20.gif' width="180" height="160"></div>
                <div id='eccv20_still'>
                    <a href='images/eccv20.gif' width="180" height="160" ><img src='images/eccv20_still.png' width="180" height="160"></a>
              </div>
                <script type="text/javascript">
                function eccv20_start() {
                  document.getElementById('eccv20').style.display = 'inline';
                  document.getElementById('eccv20_still').style.display = 'none';
                }

                function eccv20_stop() {
                  document.getElementById('eccv20').style.display = 'none';
                  document.getElementById('eccv20_still').style.display = 'inline';
                }
                eccv20_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600715.pdf">
                <papertitle>Estimating People Flows to Better Count Them in Crowded Scenes</papertitle>
              </a>
              <br>
			  <strong>Weizhe Liu</strong>, Mathieu Salzmann, Pascal Fua
              <br>
              <strong><em>The European Conference on Computer Vision (ECCV)</em>, 2020</strong>
              <br>
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600715.pdf">pdf</a> /
			  <a href="https://link.springer.com/book/10.1007/978-3-030-58452-8">press</a> /
              <a href="https://www.youtube.com/watch?v=dck3HVMLtfY">video</a> /
              <a href="https://github.com/weizheliu/People-Flows">code</a> 
              <p></p>
              <p>In this paper, we advocate estimating people flows across image locations between consecutive images and inferring the people densities from these flows instead of directly regressing. This enables us to impose much stronger constraints encoding the conservation of the number of people. As a result, it significantly boosts performance without requiring a more complex architecture. Furthermore, it also enables us to exploit the correlation between people flow and optical flow to further improve the results.</p>
            </td>
          </tr>

          <tr onmouseout="iros19_stop()" onmouseover="iros19_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class='hidden' id='iros19'><img src='images/iros19.gif' width="180" height="160"></div>
                <div id='iros19_still'>
                    <a href='images/iros19.gif' width="180" height="160" ><img src='images/iros19_still.png' width="180" height="160"></a>
              </div>
                <script type="text/javascript">
                function iros19_start() {
                  document.getElementById('iros19').style.display = 'inline';
                  document.getElementById('iros19_still').style.display = 'none';
                }

                function iros19_stop() {
                  document.getElementById('iros19').style.display = 'none';
                  document.getElementById('iros19_still').style.display = 'inline';
                }
                iros19_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1803.08805.pdf">
                <papertitle>Geometric and Physical Constraints for Drone-Based Head Plane Crowd Density Estimation</papertitle>
              </a>
              <br>
        <strong>Weizhe Liu</strong>, Krzysztof Lis, Mathieu Salzmann, Pascal Fua
              <br>
              <strong><em>The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2019</strong>
              <br>
        <a href="https://arxiv.org/pdf/1803.08805.pdf">pdf</a> /
			  <a href="https://ieeexplore.ieee.org/document/8967852">press</a> /
        <a href="https://www.youtube.com/watch?v=OLE_4Smd544">video</a>
        <p></p>
        <p>In this paper, we explicitly model the scale changes and reason in terms of people per square-meter. We show that feeding the perspective model to the network allows us to enforce global scale consistency and that this model can be obtained on the fly from the drone sensors. In addition, it also enables us to enforce physically-inspired temporal consistency constraints that do not have to be learned. This yields an algorithm that outperforms state-of-the-art methods in inferring crowd density from a moving drone camera especially when perspective effects are strong.</p>
            </td>
          </tr>		  

          <tr onmouseout="cvpr19_stop()" onmouseover="cvpr19_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class='hidden' id='cvpr19'><img src='images/cvpr19.gif' width="180" height="160"></div>
                <div id='cvpr19_still'>
                    <a href='images/cvpr19.gif' width="180" height="160" ><img src='images/cvpr19_still.png' width="180" height="160"></a>
              </div>
                <script type="text/javascript">
                function cvpr19_start() {
                  document.getElementById('cvpr19').style.display = 'inline';
                  document.getElementById('cvpr19_still').style.display = 'none';
                }

                function cvpr19_stop() {
                  document.getElementById('cvpr19').style.display = 'none';
                  document.getElementById('cvpr19_still').style.display = 'inline';
                }
                cvpr19_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Context-Aware_Crowd_Counting_CVPR_2019_paper.pdf">
                <papertitle>Context-Aware Crowd Counting</papertitle>
              </a>
              <br>
			  <strong>Weizhe Liu</strong>, Mathieu Salzmann, Pascal Fua
              <br>
              <strong><em>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019</strong>
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Context-Aware_Crowd_Counting_CVPR_2019_paper.pdf">pdf</a> /
			  <a href="https://ieeexplore.ieee.org/document/8954153">press</a> /
              <a href="https://www.youtube.com/watch?v=vEmEvs6fYyA">video</a> /
              <a href="https://github.com/weizheliu/Context-Aware-Crowd-Counting">code</a> 
              <p></p>
              <p> In this paper, we introduce an end-to-end trainable deep architecture that combines features obtained using multiple receptive field sizes and learns the importance of each such feature at each image location. In other words, our approach adaptively encodes the scale of the contextual information required to accurately predict crowd density. This yields an algorithm that outperforms state-of-the-art crowd counting methods, especially when perspective effects are strong.</p>
            </td>
          </tr>
    </table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Professional Services</heading>
        <p>
          Reviewer of major computer vision conferences (CVPR, ICCV, ECCV) and journals (TPAMI, IJCV, TIP). </p>
      </td>
    </tr>
  </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Teaching</heading>
        <p>
          <ul>
            <li> <a href="https://edu.epfl.ch/coursebook/en/introduction-to-machine-learning-ba3-CS-233-A">CS-233(a), Introduction to machine learning(BA3)</a> </li>
            <li> <a href="https://edu.epfl.ch/coursebook/fr/introduction-to-machine-learning-ba4-CS-233-B">CS-233(b), Introduction to machine learning (BA4)</a> </li>
            <li> <a href="http://isa.epfl.ch/imoniteur_ISAP/!itffichecours.htm?ww_i_matiere=1775642&ww_x_anneeacad=1866895046&ww_i_section=945571&ww_i_niveau=6683117&ww_c_langue=en">MATH-233, Probabilities and statistics </a> </li>
            <li> <a href="http://isa.epfl.ch/imoniteur_ISAP/!itffichecours.htm?ww_i_matiere=1705590&ww_x_anneeacad=1866893861&ww_i_section=249847&ww_i_niveau=6683111&ww_c_langue=en">MATH-101(e), Analysis I</a> </li>

          </ul>
        </p>
      </td>
    </tr>
  </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="1">
           <a href="https://jonbarron.info/">website template credit</a>
	    </font>
        </p>
        </td>
      </tr>
      </table>

</body>

</html>
